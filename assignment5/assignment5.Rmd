---
title: "Assignment 5"
author: "Sukhjinder Singh"
date: "30 October 2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Data Collection
First you will need to gather your data from the web API for your newspaper of choice. Because most web APIs return JSON objects, you may want to use the fromJSON function in the RJSONIO package to make your API requests. I suggest using the Guardian, which has fairly lenient query limits, and makes getting an API key very simple (https://openplatform.theguardian.com/access/), but you may use any paper you like, so long as it includes section headings with its articles and has at least six sections. You may also want to choose a paper that lets you download full article text, as some papers like the New York Times only give you the lead paragraph. To get full credit for this portion you must include your full code and demonstrate (by showing summaries of your data) that you have collected approximately 6,000 articles from a newspaper in six sections, and that approximately 1,000 articles are in each section.


```{r}
library(httr)
library(rjson)
library(lubridate)
sections <- c("books","football","lifeandstyle","us-news","world")

apiKey <- "06e9d99e-81ff-411f-b68c-f4aac4720d1f"
# Create url for each desk
urls <- lapply(sections, function(x) 
{
  paste0("http://content.guardianapis.com/search?section=", x, paste0("&order-by=newest&show-fields=body&page-size=50&api-key=", apiKey))
})

# Write a function to get results

getRes <- function(url, pageNum) 
  {

    # If pagenum is > 1, then add
  if(pageNum > 1) 
    {
    url <- paste0(url, "&page=", pageNum)
    }  
  
  # Query
  res <- content(GET(url), "parsed")
  
  # Control
  if(length(res) == 0) 
    {
    warning("Empty result. Returning  NULL.")
    return(NULL)
    }

    # Simmer down
    res <- res[["response"]]$results
  

    # Return
    return(res)
}

# Main function
mainCall <- function(url, pages = 50)
{
  # master
  master <- vector("list", pages)
  # Loop
  for(page in 1:pages) 
  {
    temp <- getRes(url, page)
    # add
    master[[page]] <- temp
    #Sys.sleep(0.5)
  }
  # Return
  return(master)
}

# For each url, call Main
data <- lapply(urls, function(x) 
  { 
  print(paste0("Section ", x))
  
  # Set limit (6000 articles)
  upperLim <- 6000
  limitPP <- 120
  
  # Number of calls needed
  calls <- upperLim / (limitPP)
  getwd()
  # Run main
  main <- mainCall(x, calls)
  # Get name
  nam <- gsub("http://content.guardianapis.com/search?section=",
              "",
              unlist(strsplit(urls[[1]], "&order", fixed=T))[1],
              fixed=T)
  
  # Write to disk
  save(main, file = paste0(nam,"_guardian.Rdata"))
})

# Load all
folder <- paste0(getwd(),"/")
files <- paste0(folder, list.files(folder))

# Vector
master <- vector("list", length(files))

# Loop
for(x in 1:length(files))
  {
  load(file=files[x])
  master[[x]] <- main
}

# To data frame
library(plyr)
library(data.table)

# Function to turn json into df
toDF <- function(json)
  {
  # return in df format
  res <- lapply(json, function(x)
  {
    # lapply
    res <- lapply(x, function(b)
      {
      # lapply
      tmp <- lapply(b, function(y) 
        {
        striphtml <- function(htmlString) 
          {
          return(gsub("<.*?>", "", htmlString))
           }
        # Return fields
        y$fields <- striphtml(y$fields$body)
        temp <- lapply(y, Filter, f = Negate(is.null))
        temp <- list("url" = ifelse(length(temp$webUrl) > 0, temp$webUrl, NA),
                     "headline" = ifelse(length(temp$webTitle) > 0, temp$webTitle, NA),
                     "body" = ifelse(length(temp$fields) > 0, temp$fields, NA),
                     "section" = ifelse(length(temp$sectionName) > 0, temp$sectionName, NA))
        # To ASCII
        temp$body <- iconv(temp$body, "latin1", "ASCII", sub="")
        # Return
        return(temp)
      })
      # Bind
      return(rbindlist(tmp, fill=T))
    })
    return(rbindlist(res, fill=T))
  })
  return(rbindlist(res, fill=T))
}

# Convert variables
res$section <- as.factor(res$section)

# Object name
final.data <- res

# Save as Rdata
save(final.data, file = "guardian_final.Rdata")

```

#2. Data cleaning (10%)
The data you have now collected is likely very dirty. Examples of issues you are likely to run across are invalid characters, HTML tags, links and other non-article text, varied cases and punctuation. All of these will cause problems for the tokenization step that comes next. For this
portion, you will clean up the article bodies you collected by removing the above mentioned messiness, and any other messiness you come across in your data. When you are finished, each article body should contain only plain, lower case text and no punctuation. 

```{r}
# Clean wd
rm(list=ls())
# Load final data
load(file="guardian_final.Rdata")
# To df
final.data <- as.data.frame(final.data)
final.data <- unique(final.data)
# Rearrange
final.data <- final.data[,c(1,4,2,3)]

# Function to fix whitespace
whiteSpaceFix <- function(string) 
  {
  
  # Strip punctuation
  temp <- gsub("[[:punct:]]", "", string)
  # Take sentence apart
  temp <- unlist(strsplit(temp, " "))
  # Control statement. If the result of the above is an empty character, then return NULL
  if(length(temp) == 0) {
    # Print message
    print("Empty character. Moving on . . . ")
    # Return empty character
    return("")
  } else{
    # Take out whitespaces
    temp <- temp[sapply(temp, function(b) b != "")]
    # Reconstruct and take out punctuation + newlines etc.
    checkF <- function(z) grepl("[[:punct:]]", z) | grepl("[\r\n\t]", z)
    temp <- temp[!checkF(temp) == TRUE]
    # Paste & collapse
    paste0(temp, collapse = " ") 
  }
}
# Set character encoding
final.data$body <- iconv(final.data$body, "latin1", "ASCII", sub="")
final.data$headline <- iconv(final.data$headline, "latin1", "ASCII", sub="")
# Take out NA's
final.data <- final.data[!is.na(final.data$body), ]
final.data <- final.data[!is.na(final.data$headline), ]
final.data <- final.data[!nchar(final.data$headline) < 2, ]
final.data <- final.data[!nchar(final.data$body) < 2, ]
# whitespaces
final.data$body <- sapply(final.data$body, whiteSpaceFix)
final.data$headline <- sapply(final.data$headline, whiteSpaceFix)
# Shuffle rows
set.seed(215)
final.data <- final.data[sample(nrow(final.data)),]
# Split data
library(caret)
set.seed(9)
index <- createDataPartition(final.data$section, p=0.9, list=F)
# Training data
samp <- final.data[index,]
summary(samp$section)
# Write to file
write.table(samp, file = "train.txt", sep="\t", row.names=F, col.names = F, fileEncoding = 'UTF-8')

# Test data
sampT <- final.data[-index,]
write.table(sampT, file = "test.txt", sep="\t", row.names=F, col.names = F)

```

3