---
title: "Assignment No 03"
author : "Pandya, Priyank Umesh"
date: "September 27, 2018"
output: pdf_document
---
## --------------------------------------------------------------------------------------------------------
## priyank.pandya@wsu.edu		                                                            
## WSU ID: 11598627
## CptS_575 Data Science                                                             
## --------------------------------------------------------------------------------------------------------

## Question 1. For this question you will be using the dplyr package to manipulate and clean up a dataset called msleep (mammals sleep) that is available on the course webpage (at https://scads.eecs.wsu.edu/wp-content/uploads/2017/10/msleep_ggplot2.csv). The dataset contains the sleep times and weights for a set of mammals. It has 83 rows and 11 variables. Here is a description of the variables:

## a) Use filter() to count the number of animals which weigh over 50 kilograms and sleep more than 6 hours a day.   
## Solution: 
```{r}
library(dplyr)
```

```{r}
msleep <- read.csv("https://scads.eecs.wsu.edu/wp-content/uploads/2017/10/msleep_ggplot2.csv")
```

```{r}
summary(msleep)
```


## b) Use piping (%>%), select() and arrange() to print the name, order, sleep time and body weightof the animals with the top 6 sleep times, in order of sleep time. 
## Solution:

```{r}
head(msleep)
```

A). Use filter() to count the number of animals which weigh over 50 kilograms and sleep more than 6 hours a day

```{r}
filter(msleep, sleep_total >6, bodywt > 50)
```
B). Use piping (%>%), select() and arrange() to print the name, order, sleep time and bodyweight of the animals with the top 6 sleep times, in order of sleep time.


```{r echo=TRUE}
msleep %>% 
    select(name, order, sleep_total,bodywt) %>%
    arrange(order, sleep_total) %>% 
    filter(sleep_total >= 17.4)
```
C). Use mutate to add two new columns to the dataframe; wt_ratio with the ratio of brain size to body weight, rem_ratio with the ratio of rem sleep to sleep time. If you think they might be useful, feel free to extract more features than these, and describe what they are

```{r}
msleep %>% 
    mutate(rem_ratio = sleep_rem / sleep_total, 
           wt_ratio = brainwt/bodywt ) %>%
    head
```

d). Use group_by() and summarize() to display the average, min and max sleep times for each order. Remember to use ungroup() when you are done.

```{r}
msleep %>% 
    group_by(order) %>%
    summarise(avg_sleep = mean(sleep_total), 
              min_sleep = min(sleep_total), 
              max_sleep = max(sleep_total),
              total = n()) %>%
              ungroup()
```

e).Make a copy of your dataframe, and use group_by() and mutate() to impute the missing brain weights as the average wt_ratio for that animal’s order times the animal’s weight. 

```{r}
Firstcopy=data.frame(msleep)
summary(Firstcopy)
```

```{r}
Firstcopy %>% 
            group_by(order) %>%
            mutate(brainwt = ifelse(is.na(brainwt),((mean(brainwt, na.rm = TRUE)/mean(bodywt, na.rm =  TRUE)) * bodywt), brainwt))
```
Make a 2 second copy of your dataframe, but this time use group_by() and mutate() to impute missing brain weights with the average brain weight for that animal’s order. 
```{r}
secondcopy=data.frame(msleep)

secondcopy %>% 
            group_by(order) %>%
           mutate( brainwt = ifelse(is.na(brainwt), mean(brainwt, na.rm = TRUE), brainwt))
```

What assumptions do these data filling methods make? Which is the best way to impute the data, or do you see a better way, and why? You may impute or remove other variables as you find appropriate. Briefly explain your decisions.

These assumptions made by mutate method based on the NA or NAN values in the dataset. There are various ways to impute the data in order to see the better result such as we can use if else statement for the computation and we can use other function such as omit to remove anomaly data. Another method could be replacing  the zeros or empty values filled by gather operation. 


Exercise 2

**For this question, you will first need to read section 12.6 in the R for Data Science book, here (http://r4ds.had.co.nz/tidy-data.html#case-study). Grab the dataset from the tidyr package, and tidy it as shown in the case study before answering the following questions **

```{r}
library(tidyr)
```


```{r}
readdata = read.csv("TB_notification.csv")
```

**a).Explain why this line > mutate(key = stringr::str_replace(key, "newrel", "new_rel")) is necessary to properly tidy the data. **

TThis dataset contains the following terms
  1). It can be seen that country, iso2 and iso3 are three variables that redundantly specify the country.
  2). We actually dont know about what all other columns are yet, but given the structure in the variables name (new_sp_m014, new_ep_m014, new_ep_f014) these are, not variables but it can be values. 
  
In the given dataset, we have to make some small changes to fix the format of the columns name because the names are somewhat inconsistent. As we can see in the statement instead of new_rel we have newrel it's very hard to spot this  but if we don't fix it we will get the errors in subsequent steps). Hence,by using the idea of replacing the characters "newrel" with "new_rel". This will make all variable names thoroughly consistent.

**What happens if you neglect the mutate() step? **

Two things can happen 
First one:

We can neglect the mutate step only and only if we know that all cases are new and we just parse the case type after the 3rd character. 

Second Solution

The warning is given by separate() function  “too few values”. If we check the rows for keys beginning with "newrel_", we see that sexage is missing, and type = m014.

```{r}
who1 <-  readdata %>%gather(new_sp:hiv_reg_new2, key = "key",value="cases",na.rm=TRUE)
who1 %>% count(key)
who2 <-  who1 %>% mutate(key = stringr::str_replace(key, "newrel", ""))
who3 <-  who2 %>%separate(key, c("new","type", "sexage"), sep="_")
head(who3)
```

b) How many entries are removed from the dataset when you set na.rm to true in the gather command (in this dataset). How else could those NA values be handled? Among these options, which do you think is the best way to handle those missing values for this dataset, and why?

In order to solve this question,We would have to know more about the data generation process. There are zero’s in the data, which means they may explicitly be indicating no cases. To get the zero's in the dataset, below is the r command.


```{r}
who1 <- readdata %>% gather(new_sp:hiv_reg_new2, key = "key", value = "cases", na.rm = TRUE)
View(who1)
```

How else could those NA values be handled? Among these options, which do you think is the best way to handle those missing values for this dataset, and why?

There are mainly Two R functions which deal with the NA values using Fill argument.

  1). In Spread(),fill value replaces all NA values. The fill argument only takes in one value.
  2). In complete(), all NA values are  under the different variables can be replaced by dissimilar values. The fill argument takes in a list that specifies the values to replace NA for different variables.
  
 Gather() and Spread() function are the best way to handle the missing values because we have the count for the indiviuals columns who has TRUE(NA) and False(value) counts. Now,for these missing value can be informative. when the dataset is analysed , It has been found that most countries have loads of missing values ! we cn decide to remove all the missing values from dataset using readdata very easily with na.omit().

c) Explain the difference between an explicit and implicit missing value, in general. Can you find any implicit missing values in this dataset, if so where? 
In the dataset, a value can be missing in the two possible ways.
  1). Explicitly which means dataset can be flagged with "NA" values which we have in this dataset as i showed in the previous example.
  2). Implicitly which means simply nothing present in the dataset. for example, it could be one or more empty row or has zero in the country column in this dataset.
  

```{r}
  who1 %>% 
  filter(cases == 0) %>% 
  nrow()
```

d).Looking at the features (country, year, var, sex, age, cases) in the tidied data, are they all appropriately typed? Are there any features you think would be better suited as a different type? Why or why not?


```{r}
who4 <- who3 %>%select(-new, -iso2,-iso3)
who5 <- who4  %>% separate(sexage, c("Sex","Age"), sep = 1)
head(who5)
```



Explain in your own words what a gather operation is, and give an example of a situation when it might be useful. Do the same for spread

Gather operation will take multiple columns and collapse them into key-value pairs, duplicating all other column needed. To show the gather operation, I took sample dataset from dplyr to show the gather operation as discussed above.

```{r}
head(mtcars)
mtcars$car <- rownames(mtcars)
mtcars <- mtcars[, c(12, 1:11)]
mtcarsNew <- mtcars %>% gather(attribute, value, -car)
head(mtcarsNew)

tail(mtcarsNew)
```
As you can see, it gathers all the columns except car and places their name and value into the attritube and value column respectively.

 If we want to gather all the columns from mpg to gear and leave the carb and car columns as they are, we can do it as follows:
 
```{r}
mtcarsNew <- mtcars %>% gather(attribute, value, mpg:gear)
head(mtcarsNew)
```


Spread operation function spreads a key-value pair across multiple columns.To use spread(), pass it the name of a data frame, then the name of the key column in the data frame, and then the name of the value column. Pass the column names as they are; do not use quotes.

In the above example, We can replicate what cast does as follows:

```{r}
mtcarsSpread <- mtcarsNew %>% spread(attribute, value)
head(mtcarsSpread)
```

Generate an informative visualization, which shows something about the data. Give a brief description of what it shows, and why you thought it was interesting.

This information is quite informative and interesting because we can visulize whole bunch of data into number of different important segments. We can take important dataset points and try to visualize based on the pattern shows above.
